{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "JdFUQNA5XUbn"
   },
   "source": [
    "# <img style=\"float: left; padding-right: 10px; width: 45px\" src=\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png\"> CS109B Data Science 2: Advanced Topics in Data Science \n",
    "\n",
    "## Homework 5: Autoencoders \n",
    "\n",
    "**Harvard University**<br/>\n",
    "**Spring 2020**<br/>\n",
    "**Instructors:** Pavlos Protopapas, Mark Glickman, Chris Tanner<br/>\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "cKWDlL0JXUbs",
    "outputId": "642bd4a8-ebb6-415f-e935-337ee5ccaef6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "blockquote { background: #AEDE94; }\n",
       "h1 { \n",
       "    padding-top: 25px;\n",
       "    padding-bottom: 25px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "h2 { \n",
       "    padding-top: 10px;\n",
       "    padding-bottom: 10px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "\n",
       "div.exercise {\n",
       "\tbackground-color: #ffcccc;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "}\n",
       "\n",
       "span.sub-q {\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.theme {\n",
       "\tbackground-color: #DDDDDD;\n",
       "\tborder-color: #E9967A; \t\n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 18pt;\n",
       "}\n",
       "div.gc { \n",
       "\tbackground-color: #AEDE94;\n",
       "\tborder-color: #E9967A; \t \n",
       "\tborder-left: 5px solid #800080; \n",
       "\tpadding: 0.5em;\n",
       "\tfont-size: 12pt;\n",
       "}\n",
       "p.q1 { \n",
       "    padding-top: 5px;\n",
       "    padding-bottom: 5px;\n",
       "    text-align: left; \n",
       "    padding-left: 5px;\n",
       "    background-color: #EEEEEE; \n",
       "    color: black;\n",
       "}\n",
       "header {\n",
       "   padding-top: 35px;\n",
       "    padding-bottom: 35px;\n",
       "    text-align: left; \n",
       "    padding-left: 10px;\n",
       "    background-color: #DDDDDD; \n",
       "    color: black;\n",
       "}\n",
       "</style>\n",
       "\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RUN THIS CELL \n",
    "import requests\n",
    "from IPython.core.display import HTML\n",
    "styles = requests.get(\"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css\").text\n",
    "HTML(styles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "cKWDlL0JXUbs",
    "outputId": "642bd4a8-ebb6-415f-e935-337ee5ccaef6"
   },
   "outputs": [],
   "source": [
    "#RUN THIS CELL\n",
    "import os\n",
    "import pathlib\n",
    "working_dir = pathlib.Path().absolute()\n",
    "os.chdir(working_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<hr style=\"height:2pt\">\n",
    "\n",
    "### INSTRUCTIONS\n",
    "\n",
    "- To submit your assignment follow the instructions given in Canvas.\n",
    "\n",
    "- This homework can be submitted in pairs.\n",
    "\n",
    "- Please restart the kernel and run the entire notebook again before you submit.\n",
    "\n",
    "<hr style=\"height:2pt\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "deletable": false,
    "editable": false,
    "id": "hakVc8z8aGmt",
    "outputId": "8ad99d3a-22a6-4509-a417-c679a16a50ca"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pylab as plt \n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Flatten, Activation, Reshape\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "rUkgUGwJXUcH"
   },
   "source": [
    "<div class=\"theme\"> Overview </div> \n",
    "\n",
    "\n",
    "In this homework, we will investigate autoencoders, how they are related to PCA (and in doing so, show that they can be a more powerful extension of this technique), and one possible application of autoencoders for outlier detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "kX9cfAwWXUcL"
   },
   "source": [
    "<div class='exercise'><b> Question 1: Autoencoders and MNIST [50pts total] </b></div>\n",
    "\n",
    "For this question, we will be using the [MNIST Dataset](https://en.wikipedia.org/wiki/MNIST_database) of handwritten digits, a simple standardized image dataset. The dataset consists of single-channel (black and white) 28x28 images, containing one digit each. We will see if it is feasible to encode (compress, in this case) the images into just 2 dimensions, a substantial compression ratio considering that the original vector has dimension 28x28=784.\n",
    "\n",
    "**1.1** [1pts] Load MNIST using `tf.keras.datasets.mnist.load_data()`, saving the training data as `x_train` and `y_train`, and the test data as `x_test`, `y_test`. Normalize the images to the range [0.,1.] by dividing by 255.\n",
    "\n",
    "**1.2** [1pts] Use `imshow` to show one image of your choice from the train set, specifying `cmap='gray'` to show the image in black and white.\n",
    "\n",
    "**1.3** [2pts] Construct and instance of `sklearn`'s `PCA` class, specifying that it only use the first 2 PCA components. Fit to `x_train` (Hint: you will need to use `reshape`), and project `x_train` down to its first 2 PCA components, saving the new array of shape (N,2) to `pca_latent_train`. This is the representation of all the images in `x_train` in a 2D latent space.\n",
    "\n",
    "**1.4** [2pts] Make a scatterplot of `pca_latent_train` with the point color designated by the corresponding class labels. Pick a reasonable color palette with enough of a contrast to clearly distinguish classes. \n",
    "\n",
    "**1.5** [8pts] Linear Autoencoder. Construct an encoder-decoder network with **linear activations** only, and **no biases**. The encoder and decoder should consist of one dense layer each, and the bottleneck dimension should be 2. The encoder and decoder should be their own separate models called `linear_encoder` and `linear_decoder`. Create the full linear autoencoder, call it `lae`, out of the encoder and decoder. Use a mean-squared-error reconstruction loss. Print the `summary()` for both the encoder and decoder networks, as well as the summary for the full linear autoencoder.\n",
    "\n",
    "**1.6** [4pts] Train your linear autoencoder `lae` on the train data, using `x_test` as validation data. Use enough epochs such that the training loss plateaus. Plot the train loss and validation (equivalent to test, in this case) loss as a function of epoch, in the same figure.\n",
    "\n",
    "**1.7** [3pts] Compute the `linear_encoder`'s latent space representation of `x_train`, calling the resulting array `lae_latent_train`. Create two scatterplots, side by side, using `subplots`, showing `pca_latent_train` (from **1.4**) and `lae_latent_train`, with points colored according to class label. Don't forget to title the two figures.\n",
    "\n",
    "**1.8** [3pts] What do you notice about the latent space representations in PCA and the LAE (linear autoencoder)? Does either one do a substantially better job at separating the 10 classes in the 2D latent space? --- *Bonus, but for no additional points: prove a relationship between the latent space representation in PCA and LAE for the same bottleneck dimension.*\n",
    "\n",
    "**1.9** [3pts] What do you expect to happen if you added more dense layers (no biases) with only linear activations to your `linear_encoder` and `linear_decoder`? Would you expect a better reconstruction error?\n",
    "\n",
    "**1.10** [8pts] Construct a nonlinear (regular) autoencoder with at least 2 dense layers, with biases, in both the encoder and decoder parts. Call the encoder network `encoder` and the decoder network `decoder`, and the full autoencoder `ae`. Print the summaries for `encoder`, `decoder`, and `ae`.\n",
    "\n",
    "**1.11** [4pts] Train your autoencoder on `x_train`, using `x_test` as validation data. Train it for a reasonable number of epochs, using your best judgement on what that entails. As usual, plot the train loss and validation loss as a function of epoch.\n",
    "\n",
    "**1.12** [3pts] Compute the `encoder`'s latent space representation of `x_train`, calling the resulting array `ae_latent_train`. Plot the scatterplots of `pca_latent_train`, `lae_latent_train`, and `ae_latent_train` in a row using `subplots` so we can see them all simultaneously. What do you notice?\n",
    "\n",
    "**1.13** [6pts] Comparing reconstructed images. You will create arrays containing the reconstructed `x_test` using PCA, the linear autoencoder, and the regular autoencoder. For PCA, be sure to use the `pca` object you created and fit in **1.3** on the *train* data. You will project `x_test` onto its 2D latent space representation, and then convert it back, saving the result as `pca_recons_x_test`. For the linear autoencoder and the regular autoencoder, save the reconstructed `x_test` as `lae_recons_x_test` and `ae_recons_x_test` respectively. Now, you will create a 6 row by 4 column collection of subplots. Each row will correspond to an element of the test set (of your choice), with the columns being the PCA reconstruction, the LAE reconstruction, the AE reconstruction, and the original image. Be sure to title the subplots with 'PCA', 'LAE', 'AE', 'Original'. \n",
    "\n",
    "**1.14** [2pts] Finally, using `sklearn.metrics`'s `mean_squared_error`, report the average reconstruction error across the entire test set for PCA, LAE, and AE. Does the ordering agree with what you've seen in the previous questions? Does it support your conclusion in **1.8**?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "deletable": false,
    "editable": false,
    "id": "hBtmANNuuS6h"
   },
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.1",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.1** [1pts] Load MNIST using `tf.keras.datasets.mnist.load_data()`, saving the training data as `x_train` and `y_train`, and the test data as `x_test`, `y_test`. Normalize the images to the range [0.,1.] by dividing by 255.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (60000, 28, 28), X_test shape: (10000, 28, 28)\n",
      "y_train shape: (60000,), and y_test shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "(X_train, y_train),(X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "print(f'X_train shape: {X_train.shape}, X_test shape: {X_test.shape}')\n",
    "print(f'y_train shape: {y_train.shape}, and y_test shape: {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.2",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.2** [1pts] Use `imshow` to show one image of your choice from the train set, specifying `cmap='gray'` to show the image in black and white.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# your code here\n",
    "i = 10\n",
    "plt.imshow(X_train[i], cmap='gray');\n",
    "plt.title(f'MNIST image number {i}')\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.3",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.3** [2pts] Construct and instance of `sklearn`'s `PCA` class, specifying that it only use the first 2 PCA components. Fit to `x_train` (Hint: you will need to use `reshape`), and project `x_train` down to its first 2 PCA components, saving the new array of shape (N,2) to `pca_latent_train`. This is the representation of all the images in `x_train` in a 2D latent space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.4",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.4** [2pts] Make a scatterplot of `pca_latent_train` with the point color designated by the corresponding class labels. Pick a reasonable color palette with enough of a contrast to clearly distinguish classes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.5",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.5** [8pts] Linear Autoencoder. Construct an encoder-decoder network with **linear activations** only, and **no biases**. The encoder and decoder should consist of one dense layer each, and the bottleneck dimension should be 2. The encoder and decoder should be their own separate models called `linear_encoder` and `linear_decoder`. Create the full linear autoencoder, call it `lae`, out of the encoder and decoder. Use a mean-squared-error reconstruction loss. Print the `summary()` for both the encoder and decoder networks, as well as the summary for the full linear autoencoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.6",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.6** [4pts] Train your linear autoencoder `lae` on the train data, using `x_test` as validation data. Use enough epochs such that the training loss plateaus. Plot the train loss and validation (equivalent to test, in this case) loss as a function of epoch, in the same figure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.7",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.7** [3pts] Compute the `linear_encoder`'s latent space representation of `x_train`, calling the resulting array `lae_latent_train`. Create two scatterplots, side by side, using `subplots`, showing `pca_latent_train` (from **1.4**) and `lae_latent_train`, with points colored according to class label. Don't forget to title the two figures.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.8",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.8** [3pts] What do you notice about the latent space representations in PCA and the LAE (linear autoencoder)? Does either one do a substantially better job at separating the 10 classes in the 2D latent space? --- *Bonus, but for no additional points: prove a relationship between the latent space representation in PCA and LAE for the same bottleneck dimension.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.9",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.9** [3pts] What do you expect to happen if you added more dense layers (no biases) with only linear activations to your `linear_encoder` and `linear_decoder`? Would you expect a better reconstruction error?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.10",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.10** [8pts] Construct a nonlinear (regular) autoencoder with at least 2 dense layers, with biases, in both the encoder and decoder parts. Call the encoder network `encoder` and the decoder network `decoder`, and the full autoencoder `ae`. Print the summaries for `encoder`, `decoder`, and `ae`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.11",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.11** [4pts] Train your autoencoder on `x_train`, using `x_test` as validation data. Train it for a reasonable number of epochs, using your best judgement on what that entails. As usual, plot the train loss and validation loss as a function of epoch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.12",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.12** [3pts] Compute the `encoder`'s latent space representation of `x_train`, calling the resulting array `ae_latent_train`. Plot the scatterplots of `pca_latent_train`, `lae_latent_train`, and `ae_latent_train` in a row using `subplots` so we can see them all simultaneously. What do you notice?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.13",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.13** [6pts] Comparing reconstructed images. You will create arrays containing the reconstructed `x_test` using PCA, the linear autoencoder, and the regular autoencoder. For PCA, be sure to use the `pca` object you created and fit in **1.3** on the *train* data. You will project `x_test` onto its 2D latent space representation, and then convert it back, saving the result as `pca_recons_x_test`. For the linear autoencoder and the regular autoencoder, save the reconstructed `x_test` as `lae_recons_x_test` and `ae_recons_x_test` respectively. Now, you will create a 6 row by 4 column collection of subplots. Each row will correspond to an element of the test set (of your choice), with the columns being the PCA reconstruction, the LAE reconstruction, the AE reconstruction, and the original image. Be sure to title the subplots with 'PCA', 'LAE', 'AE', 'Original'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "1.14",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**1.14** [2pts] Finally, using `sklearn.metrics`'s `mean_squared_error`, report the average reconstruction error across the entire test set for PCA, LAE, and AE. Does the ordering agree with what you've seen in the previous questions? Does it support your conclusion in **1.8**?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class='exercise'><b> Question 2: Convolutional Autoencoders and Outlier Detection [50pts total] </b></div>\n",
    "\n",
    "For this question, we will be using a modified version of a subset of MNIST. We have hidden some images of handwritten letters in the dataset `data/cs109b-mnist-mix.csv` amongst thousands of handwritten digits. The dataset is provided as a csv, where each row is an image, and each column gives the value of a given pixel in a flattened 28 by 28 image. It would be very tedious to have humans flip through every image to find the letters, so instead we will exploit a neat feature of autoencoders, outlier detection. This method turns a disadvantage of autoencoders, namely, their inability to properly reconstruct data very dissimilar to what they were trained on, into an advantage. \n",
    "\n",
    "You will also be constructing a convolutional autoencoder, which tends to work a lot better for reconstructing images, all while using substantially fewer parameters.\n",
    "\n",
    "**2.1** [2pts] Load and normalize (by dividing by 255) the modified dataset from `data/cs109b-mnist-mix.csv`, and reshape it to (-1, 28, 28), saving the array as `x_cs109b`. Using `imshow` and `cmap='gray'`, plot one image of your choice from this dataset.\n",
    "\n",
    "**2.2** [15pts] Create a convolutional autoencoder called `cae`. This time you don't need separate references to the encoder and decoder parts since we only intend to use the full autoencoder. You may use a combination of `Conv2D`, `MaxPooling2D`, `Flatten`, `Dense`, `Reshape`, and `UpSampling2D` layers. You may use any number of these layers, and if you are unfamiliar with any of these layers we encourage you to look at their TF Keras documentation. You will have to experiment with an appropriate bottleneck size to complete the rest of question **2**. As always, print the `summary()` of your model.\n",
    "\n",
    "**2.3** [10pts] Train your convolutional autoencoder on `x_train` (from **problem 1**, MNIST), using `x_test` as validation data. Plot the train/validation loss versus epoch. This will adapt the convolutional autoencoder to the type of data we expect, handwritten digits.\n",
    "\n",
    "**2.4** [4pts] Pass `x_test` through your convolutional autoencoder (CAE), calling the resulting reconstruction of the dataset `cae_recons_x_test`. To see how well your CAE is performing, we will visually inspect some of its reconstructions. Make an array of subplots of 6 rows and 2 columns, with the rows being different elements of the test set (your choice) and the columns being the CAE reconstruction and the Original image. How good is the reconstruction? How does it compare to the reconstruction of your dense autoencoder from **1.13**?\n",
    "\n",
    "**2.5** [4pts] Construct an instance of `tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)` and use it to calculate the MSE reconstruction error of every element in `x_test`. Save this array as `mse_x_test`; you want this output to be a 1D array so consider the required shapes of what you feed in to the `MeanSquaredError` object. We will now pass the mystery dataset through the CAE: reconstruct `x_cs109b`, saving the result as `cae_recons_x_cs109b`. Compute the reconstruction errors, saving the result as `mse_x_cs109b`.\n",
    "\n",
    "**2.6** [5pts] Using subplots, plot the histograms of `mse_x_test` and `mse_x_cs109b` side-by-side. For the most part, do they look like they are coming from similar types of data? Pick a reasonable threshold value for reconstruction error based on the histogram for `mse_x_cs109b`; plot this threshold as a vertical line on the histogram. Be sure to explain your choice of threshold. Beyond this threshold, you will examine the images to see if you can find letters. Print how many images lie beyond this threshold.\n",
    "\n",
    "**2.7** [10pts] Use the subset of data determined by your threshold to find the letters in `x_cs109b`, displaying them as images. Show your work! Create a 2-column table indicating the letter (e.g., 'a') and the index where it is located (e.g. '9728'). There are a few letters total. To get full credit you need to find most of them (you should not need to examine more than a few tens of images - if so, consider a different threshold in **2.6** or check your CAE performance).\n",
    "\n",
    "You will have been able to find the majority of the letters hidden in the dataset of thousands of images, while only having to manually look at 1/100th of the dataset. This demonstrates how autoencoders could be used as a cheap preprocessing step to draw out the most \"interesting\" data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.1",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**2.1** [2pts] Load and normalize (by dividing by 255) the modified dataset from `data/cs109b-mnist-mix.csv`, and reshape it to (-1, 28, 28), saving the array as `x_cs109b`. Using `imshow` and `cmap='gray'`, plot one image of your choice from this dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.2",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**2.2** [15pts] Create a convolutional autoencoder called `cae`. This time you don't need separate references to the encoder and decoder parts since we only intend to use the full autoencoder. You may use a combination of `Conv2D`, `MaxPooling2D`, `Flatten`, `Dense`, `Reshape`, and `UpSampling2D` layers. You may use any number of these layers, and if you are unfamiliar with any of these layers we encourage you to look at their TF Keras documentation. You will have to experiment with an appropriate bottleneck size to complete the rest of question **2**. As always, print the `summary()` of your model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.3",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**2.3** [10pts] Train your convolutional autoencoder on `x_train` (from **problem 1**, MNIST), using `x_test` as validation data. Plot the train/validation loss versus epoch. This will adapt the convolutional autoencoder to the type of data we expect, handwritten digits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.4",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**2.4** [4pts] Pass `x_test` through your convolutional autoencoder (CAE), calling the resulting reconstruction of the dataset `cae_recons_x_test`. To see how well your CAE is performing, we will visually inspect some of its reconstructions. Make an array of subplots of 6 rows and 2 columns, with the rows being different elements of the test set (your choice) and the columns being the CAE reconstruction and the Original image. How good is the reconstruction? How does it compare to the reconstruction of your dense autoencoder from **1.13**?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.5",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**2.5** [4pts] Construct an instance of `tf.keras.losses.MeanSquaredError(reduction=tf.keras.losses.Reduction.NONE)` and use it to calculate the MSE reconstruction error of every element in `x_test`. Save this array as `mse_x_test`; you want this output to be a 1D array so consider the required shapes of what you feed in to the `MeanSquaredError` object. We will now pass the mystery dataset through the CAE: reconstruct `x_cs109b`, saving the result as `cae_recons_x_cs109b`. Compute the reconstruction errors, saving the result as `mse_x_cs109b`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.6",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**2.6** [5pts] Using subplots, plot the histograms of `mse_x_test` and `mse_x_cs109b` side-by-side. For the most part, do they look like they are coming from similar types of data? Pick a reasonable threshold value for reconstruction error based on the histogram for `mse_x_cs109b`; plot this threshold as a vertical line on the histogram. Be sure to explain your choice of threshold. Beyond this threshold, you will examine the images to see if you can find letters. Print how many images lie beyond this threshold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "autograde": "2.7",
    "deletable": false,
    "editable": false
   },
   "source": [
    "**2.7** [10pts] Use the subset of data determined by your threshold to find the letters in `x_cs109b`, displaying them as images. Show your work! Create a 2-column table indicating the letter (e.g., 'a') and the index where it is located (e.g. '9728'). There are a few letters total. To get full credit you need to find most of them (you should not need to examine more than a few tens of images - if so, consider a different threshold in **2.6** or check your CAE performance).\n",
    "\n",
    "You will have been able to find the majority of the letters hidden in the dataset of thousands of images, while only having to manually look at 1/100th of the dataset. This demonstrates how autoencoders could be used as a cheap preprocessing step to draw out the most \"interesting\" data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false
   },
   "source": [
    "*Your answer here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "cs109b-hw3_finalDraft.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
